{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7865748,"sourceType":"datasetVersion","datasetId":4614647},{"sourceId":7866670,"sourceType":"datasetVersion","datasetId":4615338}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# Path to the directory containing training and testing data\ntrain_dir = '/kaggle/input/cutmix-17march/cutmix_new/train'\ntest_dir = '/kaggle/input/cutmix-17march/cutmix_new/test'\n\n# Image dimensions\nimg_height = 299\nimg_width = 299\nbatch_size = 32\n\n# Preprocessing function for InceptionV3\npreprocess_input = tf.keras.applications.inception_v3.preprocess_input\n\n# Create data generators for training and testing\ntrain_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary')\n\ntest_generator = test_datagen.flow_from_directory(\n    test_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='binary')\n\n# Load the pre-trained InceptionV3 model without the top classification layer\ninception_base = InceptionV3(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n\n# Freeze the layers of the pre-trained InceptionV3\nfor layer in inception_base.layers:\n    layer.trainable = False\n\n# Add custom classification layers on top of InceptionV3\nx = layers.GlobalAveragePooling2D()(inception_base.output)\nx = layers.Dense(256, activation='relu')(x)\noutput = layers.Dense(1, activation='sigmoid')(x)\n\n# Create the model\nmodel = models.Model(inputs=inception_base.input, outputs=output)\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(train_generator,\n                    steps_per_epoch=None,  # Set to None to run until the generator stops\n                    epochs=10,\n                    validation_data=test_generator,\n                    validation_steps=None)  # Set to None to run until the generator stops\n\n# Evaluate the model\nloss, accuracy = model.evaluate(test_generator)\nprint(\"Test Accuracy:\", accuracy)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-17T13:11:10.724628Z","iopub.execute_input":"2024-03-17T13:11:10.725391Z","iopub.status.idle":"2024-03-17T14:16:50.993537Z","shell.execute_reply.started":"2024-03-17T13:11:10.725354Z","shell.execute_reply":"2024-03-17T14:16:50.989988Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-17 13:11:13.444317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-17 13:11:13.444462: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-17 13:11:13.627224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Found 1294 images belonging to 2 classes.\nFound 856 images belonging to 2 classes.\nDownloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m87910968/87910968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nEpoch 1/10\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 8s/step - accuracy: 0.5469 - loss: 0.9098 - val_accuracy: 0.7453 - val_loss: 0.5178\nEpoch 2/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 8s/step - accuracy: 0.7476 - loss: 0.5320 - val_accuracy: 0.8306 - val_loss: 0.4117\nEpoch 3/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m404s\u001b[0m 8s/step - accuracy: 0.7814 - loss: 0.4564 - val_accuracy: 0.8551 - val_loss: 0.3704\nEpoch 4/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m368s\u001b[0m 8s/step - accuracy: 0.8071 - loss: 0.3871 - val_accuracy: 0.8458 - val_loss: 0.3567\nEpoch 5/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 8s/step - accuracy: 0.8719 - loss: 0.3244 - val_accuracy: 0.8493 - val_loss: 0.3460\nEpoch 6/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m382s\u001b[0m 8s/step - accuracy: 0.8720 - loss: 0.3213 - val_accuracy: 0.8621 - val_loss: 0.3241\nEpoch 7/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m389s\u001b[0m 8s/step - accuracy: 0.8806 - loss: 0.2862 - val_accuracy: 0.8563 - val_loss: 0.3204\nEpoch 8/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m375s\u001b[0m 8s/step - accuracy: 0.9004 - loss: 0.2538 - val_accuracy: 0.8703 - val_loss: 0.3145\nEpoch 9/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m381s\u001b[0m 8s/step - accuracy: 0.9131 - loss: 0.2499 - val_accuracy: 0.8692 - val_loss: 0.3173\nEpoch 10/10\n\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 8s/step - accuracy: 0.9094 - loss: 0.2154 - val_accuracy: 0.8657 - val_loss: 0.3339\n\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 5s/step - accuracy: 0.8759 - loss: 0.3263\nTest Accuracy: 0.8656542301177979\n","output_type":"stream"}]},{"cell_type":"code","source":"image_path = '/kaggle/input/monalisa-predict-cutmix/monalisa.jpeg'\n\nimg = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width))\nimg_array = tf.keras.preprocessing.image.img_to_array(img)\nimg_array = np.expand_dims(img_array, axis=0)\nimg_array = preprocess_input(img_array)\n\nprediction = model.predict(img_array)\n\nif prediction < 0.5:\n    print(\"AI generated Image\")\nelse:\n    print(\"Original Image\")","metadata":{"execution":{"iopub.status.busy":"2024-03-17T14:57:41.039828Z","iopub.execute_input":"2024-03-17T14:57:41.040497Z","iopub.status.idle":"2024-03-17T14:57:44.642493Z","shell.execute_reply.started":"2024-03-17T14:57:41.040389Z","shell.execute_reply":"2024-03-17T14:57:44.641031Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\nOriginal Image\n","output_type":"stream"}]}]}